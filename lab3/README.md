# Урок 3: Полносвязные сети, MNIST и CIFAR

В этом уроке мы:
- Разберемся с обратным распространением ошибки и графом вычислений
- Научимся работать с готовыми PyTorch слоями
- Создадим простые классификаторы для MNIST и CIFAR
- Изучим различные архитектуры полносвязных сетей
- Познакомимся с техниками регуляризации (Dropout, BatchNorm)

## Структура урока

### Теоретическая часть
- **Обратное распространение ошибки** - математические основы
- **Граф вычислений** - как PyTorch строит и отслеживает граф
- **Полносвязные сети** - архитектура и принципы работы

### Практическая часть

#### Папка `fully_connected_basics/`
- `datasets.py` - PyTorch Dataset обертки для MNIST и CIFAR
- `models.py` - полносвязные модели с конфигурацией через JSON
- `trainer.py` - компактные обучающие и тестирующие циклы
- `utils.py` - вспомогательные функции

## Основные концепции

### Полносвязные сети (Fully Connected Networks)
- Линейные слои (nn.Linear)
- Функции активации (ReLU, Sigmoid, Tanh)
- Dropout для регуляризации
- Batch Normalization для стабилизации обучения

### Датасеты
- **MNIST**: 28x28 пикселей, 10 классов (цифры 0-9)
- **CIFAR-10**: 32x32x3 пикселей, 10 классов (объекты)

### Архитектурные эксперименты
- Влияние количества слоев
- Влияние количества нейронов в слоях
- Эффективность различных функций активации
- Роль регуляризации

## Требования
См. `requirements.txt`